
#  Agentic Test Case Generator with Conversational Refinement

**Generate structured, traceable test cases from user stories and specifications â€” and refine them via natural language conversation.**

Built with [Streamlit](https://streamlit.io), [LangGraph](https://docs.langchain.com/langgraph/), and powered by [Groq](https://groq.com), this app streamlines test case creation by combining intelligent automation with human feedback.

---

## ğŸš€ Features

- ğŸ“„ **Upload PDFs** containing user stories and technical specifications.
- ğŸ§  **LLM-powered generation** of detailed test cases for both.
- ğŸ”„ **Smart merging** of overlapping or duplicate tests.
- ğŸ”— **Traceability** â€” track test case origin (User Story, Spec, or Both).
- ğŸ’¬ **Conversational refinement** using simple prompts (e.g., â€œAdd a negative test case for loginâ€).

---

## ğŸ› ï¸ Tech Stack

- **Frontend**: Streamlit
- **Backend Orchestration**: LangGraph
- **LLM Provider**: Groq (LLaMA3 via `langchain_groq`)
- **PDF Parsing**: PyPDF2

---

## ğŸ“¦ Installation

1. **Clone the Repository**

```bash
git clone https://github.com/your-username/agentic-test-case-generator.git
cd agentic-test-case-generator
```

2. **Install Dependencies**

We recommend using a virtual environment:

```bash
pip install -r requirements.txt
```

_Example `requirements.txt`:_

```text
streamlit
PyPDF2
langgraph
langchain
langchain-groq
```

3. **Run the App**

```bash
streamlit run main.py
```

---

## ğŸ” Authentication

The app requires a **Groq API key**.

- Youâ€™ll be prompted to enter the API key in the **sidebar**.
- You can clear the stored key via the "Logout" button.

Get your key from [Groq Console](https://console.groq.com).

---

## ğŸ“‚ How It Works

### 1. Upload PDFs
- One for **user stories**
- One for **technical specifications**

### 2. Click â€œGenerate Test Casesâ€
- Extracts text from PDFs
- Creates test cases from each source
- Merges and deduplicates them
- Adds a traceability column

### 3. Chat with the AI
Use natural language prompts like:

- â€œAdd more edge cases.â€
- â€œInclude performance test scenarios.â€
- â€œRemove tests related to password reset.â€

The AI refines the current list of test cases accordingly.

---

## ğŸ§± Project Structure

```
.
â”œâ”€â”€ main.py             # Streamlit UI + interaction logic
â”œâ”€â”€ nodes.py            # LLM prompt functions (test generation, merge, refine)
â”œâ”€â”€ graph.py            # LangGraph workflow definition
â”œâ”€â”€ state.py            # State schema for LangGraph
â””â”€â”€ README.md
```

---

## âœ… Example Use Case

Upload:
- `user_stories.pdf`: describing a login system
- `spec.pdf`: outlining technical requirements for auth flow

The app will generate a structured markdown test suite like:

| Test Case ID | Description | Steps | Expected Results | Source |
|--------------|-------------|-------|------------------|--------|
| TC-01        | Valid login | 1. Go to login page... | User is logged in | Both |

Then ask:
> â€œAdd a test for password expiration handling.â€

And it will update the test suite accordingly.

---

## ğŸ¤ Contributing

Pull requests are welcome! Feel free to open issues for bugs, improvements, or ideas.

---

## ğŸŒ Acknowledgments

- [LangChain](https://www.langchain.com/)
- [LangGraph](https://docs.langchain.com/langgraph/)
- [Groq](https://groq.com)
- [Streamlit](https://streamlit.io)
